from datetime import datetime
from pydantic import BaseModel, Field
from langgraph.graph import START, END, StateGraph
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_deepseek import ChatDeepSeek
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from langchain_core.messages import SystemMessage
from langchain_core.utils.function_calling import convert_to_openai_function
from state import classState

load_dotenv()

llm = ChatDeepSeek(
    model="deepseek-chat",  
    temperature=0.1,
    max_tokens=4000
)

def buildSearch(questionUser):
    print("üöÄ Iniciando a constru√ß√£o das perguntas de pesquisa...")
    
    systemPrompt = """\
    # Papel:
    Voc√™ √© um pesquisador experiente com a habilidade de criar perguntas de pesquisa claras e eficazes para buscas na web.

    # Objetivo
    Seu objetivo √© gerar UMA pergunta de pesquisa com base na pergunta do usu√°rio.

    # Contexto
    - A data atual √©: {data_atual}

    # Aten√ß√£o
    - A pergunta deve ser formulada em linguagem natural, como um humano digitaria no Google.
    - A pergunta deve ser uma string simples, sem aspas ou formata√ß√£o especial.

    # Pergunta do usu√°rio
    {questionUser}
    
    # Tarefa:
    Com base na pergunta do usu√°rio, gere de 3 perguntas de pesquisa otimizadas para encontrar fontes de alta qualidade na web.
    """
    class searchBase(BaseModel):
        """
        Define a estrutura de dados para as perguntas de pesquisa geradas.
        """
        searchQuestions: list[str] = Field(
            description="Uma lista Python contendo 3 perguntas de pesquisa."
        )

    systemPrompt = systemPrompt.format(data_atual=datetime.now().strftime("%d/%m/%Y"), questionUser=questionUser)
    llm_structure = llm.with_structured_output(searchBase)
    result = llm_structure.invoke(systemPrompt) 
    print("‚úÖ Perguntas de pesquisa constru√≠das com sucesso!\n")
    return result.searchQuestions

def summaryContentSearch(contentList):
    print("üìù Iniciando a gera√ß√£o de resumos dos conte√∫dos...")
    systemPrompt = """\
    # Papel:
    Voc√™ √© um assistente de pesquisa que deve resumir o conte√∫do de artigos.

    # Tarefa:
    Gere um resumo conciso e informativo para cada artigo na lista de conte√∫do.
    
    # Conte√∫dos:
    {content_list}
    """
    class summaryBase(BaseModel):
        summaries: str = Field(
            description="Retornar um resumo completo de todos os conte√∫dos que recebeu em seu no prompt, reusmo completo com todas as informa√ß√µes mais importantes de todos os artigos."
        )
    systemPrompt = systemPrompt.format(content_list=contentList)
    llm_structure = llm.with_structured_output(summaryBase)
    try:
        result = llm_structure.invoke(systemPrompt)
        if result is None:
            print("‚ùå ERRO: O LLM n√£o retornou um resultado v√°lido.")
            return "N√£o foi poss√≠vel gerar o resumo."
        print("‚úÖ Resumos gerados com sucesso!\n")
        return result.summaries
    except Exception as e:
        print(f"‚ùå Erro ao gerar resumo: {e}")
        return "Erro ao gerar resumo."

def createContext(summaryContents):
    print("üß† Iniciando a cria√ß√£o do contexto a partir dos resumos...")
    systemPrompt = """\
    # Papel:
    Voc√™ √© um analisador e criador de contextos para llms, voc√™ tem o papel de analisar resumos e criar um contexto completo e informativo a partir deles.

    # Tarefa:
    Gere um contexto completo e informativo a partir dos resumos fornecidos.
    
    # Resumos:
    {summary_contents}
    """
    class contextBase(BaseModel):
        context: str = Field(
            description="Retornar um contexto completo e informativo a partir dos resumos fornecidos."
        )
    systemPrompt = systemPrompt.format(summary_contents=summaryContents)
    llm_structure = llm.with_structured_output(contextBase)
    result = llm_structure.invoke(systemPrompt)
    print("‚úÖ Contexto gerado com sucesso!\n")
    return result.context

def contextAnalysis(questionUser, context):
    print("üß† Iniciando a an√°lise do contexto...")
    systemPrompt = """\
    # Papel:
    Voc√™ √© um analisador de contexto, voc√™ tem o papel de analisar o contexto fornecido e gerar uma an√°lise completa e informativa.

    # Tarefa:
    Analisar todo o contexto fornecido, gerar uma an√°lise, e verificar se com esse contexto √© poss√≠vel responder a pergunta do usu√°rio.
    
    # Contexto:
    {context}

    # Pergunta do usu√°rio:
    {questionUser}
    """
    class analysisBase(BaseModel):
        analysis: str = Field(
            description="Retorne '0' se o contexto for suficiente para responder √† pergunta do usu√°rio, e '1' se n√£o for. A resposta deve ser apenas '0' ou '1'."
        )
    systemPrompt = systemPrompt.format(context=context,questionUser=questionUser)
    llm_structure = llm.with_structured_output(analysisBase)
    result = llm_structure.invoke(systemPrompt)
    print(f"‚úÖ An√°lise do contexto conclu√≠da. Resultado: {result.analysis}\n")
    return result.analysis

def responseGenerator(questionUser, context):
    print("üìù Iniciando a gera√ß√£o da resposta final...")
    systemPrompt = """\
    # Papel:
    Voc√™ √© um especialista em gerar respostas detalhadas e bem estruturadas. Sua principal fun√ß√£o √© analisar um contexto fornecido e, com base nele, responder √† pergunta do usu√°rio de forma completa e informativa, utilizando o formato Markdown para organizar a resposta.

    # Tarefa:
    Com base na pergunta do usu√°rio e no contexto fornecido, gere uma resposta completa e informativa em formato Markdown. A resposta deve abordar todos os principais t√≥picos presentes no contexto, organizando-os de maneira l√≥gica e clara.

    # Instru√ß√µes de Formato (Markdown):
    - Utilize cabe√ßalhos (`#`, `##`, `###`) para estruturar os diferentes t√≥picos da resposta.
    - Empregue listas (`-` ou `*`) para enumerar pontos importantes, caracter√≠sticas ou exemplos.
    - Use negrito (`**texto**`) ou it√°lico (`*texto*`) para destacar termos-chave e conceitos relevantes.
    - Se o contexto contiver dados que possam ser apresentados em formato de tabela, organize-os dessa maneira para maior clareza.

    # Processo:
    1.  **An√°lise do Contexto:** Identifique os principais temas, argumentos e informa√ß√µes presentes no texto de contexto.
    2.  **Estrutura√ß√£o da Resposta:** Organize os t√≥picos identificados em uma sequ√™ncia l√≥gica que responda √† pergunta do usu√°rio da maneira mais eficaz.
    3.  **Elabora√ß√£o do Conte√∫do:** Desenvolva cada t√≥pico, explicando os pontos importantes e citando as informa√ß√µes relevantes do contexto.
    4.  **Formata√ß√£o em Markdown:** Aplique a formata√ß√£o Markdown para garantir que a resposta seja clara, leg√≠vel e bem organizada.

    # Pergunta do usu√°rio:
    {questionUser}

    # Contexto:
    {context}
    """
    class responseBase(BaseModel):
        response: str = Field(
            description="Retorne uma resposta completa e informativa no formato md a partir da pergunta do usu√°rio e do contexto fornecido."
        )
    systemPrompt = systemPrompt.format(context=context, questionUser=questionUser)
    llm_structure = llm.with_structured_output(responseBase)
    result = llm_structure.invoke(systemPrompt)
    return result.response


    